{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "768d5f64-4aac-43f9-b645-d35020f1dc9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import libraries\n",
    "import math\n",
    "import os\n",
    "import re\n",
    "import csv\n",
    "import sys\n",
    "import io\n",
    "import hashlib\n",
    "\n",
    "import pandas as pd\n",
    "import requests\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "from decimal import Decimal\n",
    "from requests import get\n",
    "from zipfile import ZipFile\n",
    "import shutil\n",
    "import wget\n",
    "import re\n",
    "import tensorflow as tf\n",
    "#tf.enable_eager_execution()\n",
    "\n",
    "from pandas.io import sql\n",
    "import pyodbc\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "import codecs\n",
    "import seaborn as sns\n",
    "from dask import dataframe as df1\n",
    "\n",
    "##Import section to ensure the main directory is in the path\n",
    "#module_path = os.path.abspath(os.path.join('..'))\n",
    "#if module_path not in sys.path:\n",
    "#    sys.path.append(module_path)\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "sys.path.append('../utils/')\n",
    "from sqlalchemy import create_engine\n",
    "import datetime as dt\n",
    "\n",
    "print('Python: ' + sys.version.split('|')[0])\n",
    "print('Pandas: ' + pd.__version__)\n",
    "print('pyODBC: ' + pyodbc.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eae46410-8557-4702-9d3d-e36abcd15b66",
   "metadata": {},
   "outputs": [],
   "source": [
    "#setup SQL server config\n",
    "DB = {'servername': 'DESKTOP-FPGABNM',\n",
    "      'database': 'PAC_Data',\n",
    "      'driver': 'driver=SQL Server Native Client 11.0'}\n",
    "\n",
    "# create the connection\n",
    "engine = create_engine('mssql+pyodbc://' + DB['servername'] + '/' + DB['database'] + \"?\" + DB['driver'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f227117e-cdca-4bd8-86f3-9db6803cd43a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Unzip early voting files into temp folder\n",
    "zip_output_dir = r\"..\\\\output\\\\temp\\\\\"\n",
    "\n",
    "\n",
    "for file in os.listdir(\"..\\\\data\\\\completion_zip\"):\n",
    "    zip_file = \"..\\\\data\\\\earlyvoting\\\\\" + file\n",
    "\n",
    "    with ZipFile(zip_file) as zip_file:\n",
    "        for member in zip_file.namelist():\n",
    "            filename = os.path.basename(member)\n",
    "            # skip directories\n",
    "            if not filename:\n",
    "                continue\n",
    "\n",
    "            # copy file (taken from zipfile's extract)\n",
    "            source = zip_file.open(member)\n",
    "            target = open(os.path.join(zip_output_dir, filename), \"wb\")\n",
    "            with source, target:\n",
    "                shutil.copyfileobj(source, target)\n",
    "                \n",
    "## early voting testing\n",
    "ev_file1 = open(\"\\\\data\\\\earlyvoting\\\\\" + \"ECTOR_EV_5_1_2023.xlsx\", 'r', newline='', encoding='utf-8')\n",
    "ev_file2 = open(\"\\\\data\\\\earlyvoting\\\\\" + \"ECTOR_EV_5_25_2023.xlsx\", 'r', newline='', encoding='utf-8')\n",
    "\n",
    "#\n",
    "ectorcad_tax_roll = open(\"\\\\data\\\\ector_appraisal\\\\\" + \"Rpt_Acct_Detail.csv\", 'r', newline=None)\n",
    "#ectorcad_geodata = \"\\\\data\\\\ector_appraisal_geodata\\\\Ectorparcel_GEODB.gdb\\\\\" + \"a0000000a.spx\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23a9d82d-7091-4f87-b990-c117e81c0808",
   "metadata": {},
   "outputs": [],
   "source": [
    "##TO-DO code from well decline and spacing. Integrate into utils\n",
    "\n",
    "\"\"\"Label map utility functions.\"\"\"\n",
    "\n",
    "import logging\n",
    "\n",
    "import tensorflow as tf\n",
    "from google.protobuf import text_format\n",
    "from protos import string_int_label_map_pb2\n",
    "\n",
    "\n",
    "def _validate_label_map(label_map):\n",
    "  \"\"\"Checks if a label map is valid.\n",
    "\n",
    "  Args:\n",
    "    label_map: StringIntLabelMap to validate.\n",
    "\n",
    "  Raises:\n",
    "    ValueError: if label map is invalid.\n",
    "  \"\"\"\n",
    "  for item in label_map.item:\n",
    "    if item.id < 1:\n",
    "      raise ValueError('Label map ids should be >= 1.')\n",
    "\n",
    "\n",
    "def create_category_index(categories):\n",
    "  \"\"\"Creates dictionary of COCO compatible categories keyed by category id.\n",
    "\n",
    "  Args:\n",
    "    categories: a list of dicts, each of which has the following keys:\n",
    "      'id': (required) an integer id uniquely identifying this category.\n",
    "      'name': (required) string representing category name\n",
    "        e.g., 'cat', 'dog', 'pizza'.\n",
    "\n",
    "  Returns:\n",
    "    category_index: a dict containing the same entries as categories, but keyed\n",
    "      by the 'id' field of each category.\n",
    "  \"\"\"\n",
    "  category_index = {}\n",
    "  for cat in categories:\n",
    "    category_index[cat['id']] = cat\n",
    "  return category_index\n",
    "\n",
    "\n",
    "def convert_label_map_to_categories(label_map,\n",
    "                                    max_num_classes,\n",
    "                                    use_display_name=True):\n",
    "  \"\"\"Loads label map proto and returns categories list compatible with eval.\n",
    "\n",
    "  This function loads a label map and returns a list of dicts, each of which\n",
    "  has the following keys:\n",
    "    'id': (required) an integer id uniquely identifying this category.\n",
    "    'name': (required) string representing category name\n",
    "      e.g., 'cat', 'dog', 'pizza'.\n",
    "  We only allow class into the list if its id-label_id_offset is\n",
    "  between 0 (inclusive) and max_num_classes (exclusive).\n",
    "  If there are several items mapping to the same id in the label map,\n",
    "  we will only keep the first one in the categories list.\n",
    "\n",
    "  Args:\n",
    "    label_map: a StringIntLabelMapProto or None.  If None, a default categories\n",
    "      list is created with max_num_classes categories.\n",
    "    max_num_classes: maximum number of (consecutive) label indices to include.\n",
    "    use_display_name: (boolean) choose whether to load 'display_name' field\n",
    "      as category name.  If False or if the display_name field does not exist,\n",
    "      uses 'name' field as category names instead.\n",
    "  Returns:\n",
    "    categories: a list of dictionaries representing all possible categories.\n",
    "  \"\"\"\n",
    "  categories = []\n",
    "  list_of_ids_already_added = []\n",
    "  if not label_map:\n",
    "    label_id_offset = 1\n",
    "    for class_id in range(max_num_classes):\n",
    "      categories.append({\n",
    "          'id': class_id + label_id_offset,\n",
    "          'name': 'category_{}'.format(class_id + label_id_offset)\n",
    "      })\n",
    "    return categories\n",
    "  for item in label_map.item:\n",
    "    if not 0 < item.id <= max_num_classes:\n",
    "      logging.info('Ignore item %d since it falls outside of requested '\n",
    "                   'label range.', item.id)\n",
    "      continue\n",
    "    if use_display_name and item.HasField('display_name'):\n",
    "      name = item.display_name\n",
    "    else:\n",
    "      name = item.name\n",
    "    if item.id not in list_of_ids_already_added:\n",
    "      list_of_ids_already_added.append(item.id)\n",
    "      categories.append({'id': item.id, 'name': name})\n",
    "  return categories\n",
    "\n",
    "\n",
    "def load_labelmap(path):\n",
    "  \"\"\"Loads label map proto.\n",
    "\n",
    "  Args:\n",
    "    path: path to StringIntLabelMap proto text file.\n",
    "  Returns:\n",
    "    a StringIntLabelMapProto\n",
    "  \"\"\"\n",
    "  with tf.gfile.GFile(path, 'r') as fid: \n",
    "  # with tf.compat.v2.io.gfile.GFile(path, 'r') as fid: # use this line to run it with TensorFlow version 2.x\n",
    "    label_map_string = fid.read()\n",
    "    label_map = string_int_label_map_pb2.StringIntLabelMap()\n",
    "    try:\n",
    "      text_format.Merge(label_map_string, label_map)\n",
    "    except text_format.ParseError:\n",
    "      label_map.ParseFromString(label_map_string)\n",
    "  _validate_label_map(label_map)\n",
    "  return label_map\n",
    "\n",
    "\n",
    "def get_label_map_dict(label_map_path, use_display_name=False):\n",
    "  \"\"\"Reads a label map and returns a dictionary of label names to id.\n",
    "\n",
    "  Args:\n",
    "    label_map_path: path to label_map.\n",
    "    use_display_name: whether to use the label map items' display names as keys.\n",
    "\n",
    "  Returns:\n",
    "    A dictionary mapping label names to id.\n",
    "  \"\"\"\n",
    "  label_map = load_labelmap(label_map_path)\n",
    "  label_map_dict = {}\n",
    "  for item in label_map.item:\n",
    "    if use_display_name:\n",
    "      label_map_dict[item.display_name] = item.id\n",
    "    else:\n",
    "      label_map_dict[item.name] = item.id\n",
    "  return label_map_dict\n",
    "\n",
    "\n",
    "def create_category_index_from_labelmap(label_map_path):\n",
    "  \"\"\"Reads a label map and returns a category index.\n",
    "\n",
    "  Args:\n",
    "    label_map_path: Path to `StringIntLabelMap` proto text file.\n",
    "\n",
    "  Returns:\n",
    "    A category index, which is a dictionary that maps integer ids to dicts\n",
    "    containing categories, e.g.\n",
    "    {1: {'id': 1, 'name': 'dog'}, 2: {'id': 2, 'name': 'cat'}, ...}\n",
    "  \"\"\"\n",
    "  label_map = load_labelmap(label_map_path)\n",
    "  max_num_classes = max(item.id for item in label_map.item)\n",
    "  categories = convert_label_map_to_categories(label_map, max_num_classes)\n",
    "  return create_category_index(categories)\n",
    "\n",
    "\n",
    "def create_class_agnostic_category_index():\n",
    "  \"\"\"Creates a category index with a single `object` class.\"\"\"\n",
    "  return {1: {'id': 1, 'name': 'object'}}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ae8ad9e-8d2e-45ae-b5fa-e1502e4e476f",
   "metadata": {},
   "outputs": [],
   "source": [
    "insert_list = []\n",
    "\n",
    "#helpers need to move into utils folder after testing\n",
    "def swap_production_dates_for_time_delta(wellDF):\n",
    "    import pandas as pd\n",
    "\n",
    "    # generate new column with time delta rather than production date\n",
    "    voters_masterDF['Time Delta'] = '69 days 00:00:00'\n",
    "    voters_masterDF['Time Delta'] = pd.to_timedelta(voters_masterDF['Time Delta'])\n",
    "\n",
    "    for ID in set(voters_masterDF['SOS_VoterID']):\n",
    "        startDate = '2023_11_7' ##Election day\n",
    "        delta = voters_masterDF.loc[voters_masterDF['SOS_VoterID'] == ID, 'eligible_date'] - startDate\n",
    "        voters_masterDF.loc[wellDF['SOS_VoterID'] == API, 'Time Delta'] = delta\n",
    "\n",
    "\n",
    "    return voterDF\n",
    "\n",
    "def yyyymmdd(date):\n",
    "    date = decode_date(date)\n",
    "    #Changes format YYYYMMDD from a series of numbers to datetime object\n",
    "    try:\n",
    "        val = datetime.strptime(date, '%Y%m%d').strftime('%m/%d/%Y')\n",
    "    except ValueError:\n",
    "        val = None\n",
    "    return val\n",
    "    \n",
    "def yyyymm(yyyymm):\n",
    "    yyyymm = decode_date(yyyymm)\n",
    "    #Changes format YYYYMM from a series of numbers to datetime object\n",
    "    #makes the date the first day of the month\n",
    "    try:\n",
    "        val = date(year=int(yyyymm[0:4]), month=int(yyyymm[4:]), day=1).strftime('%m/01/%Y')\n",
    "    except ValueError:\n",
    "        val = None\n",
    "\n",
    "    return val\n",
    "\n",
    "def datefix(date):\n",
    "    date = decode_date(date)\n",
    "    #Changes format YYYYMMDD from a series of numbers to datetime object\n",
    "    try:\n",
    "        val = datetime.strptime(date, '%Y%m%d').strftime('%Y-%m-01')\n",
    "    except ValueError:\n",
    "        val = None\n",
    "    return val\n",
    "\n",
    "def numeric(num):\n",
    "    num = ebc_decode(num)\n",
    "    try: ##using a try to ensure the values passed are actually 0-9 with no other characters\n",
    "        val = int(num)\n",
    "    except:\n",
    "        val = None\n",
    "\n",
    "    return val\n",
    "\n",
    "\n",
    "\n",
    "voters_master_columns = ['SOS_VoterID', 'idnumber', 'voter_status', 'party_code', 'last_name', 'first_name', 'middle_name', 'name_suffix', 'street_number', 'street_building',\n",
    "                      'street_pre_dir', 'street_name', 'street_type', 'unit_type', 'unit', 'city', 'zip', 'zip4', 'mail_1', 'mail_2', 'mail_3', 'mail_city',\n",
    "                      'mail_state', 'mail_zip', 'mail_zip_4', 'sex', 'birth_date', 'eligible_date', 'effective_date', 'precinct', 'precsub', 'district_01', 'district_02',\n",
    "                      'district_03', 'district_04', 'district_05', 'district_06', 'district_07', 'district_08', 'district_09', 'district_10', 'district_11', 'district_12',\n",
    "                      'district_13', 'district_14', 'district_15', 'district_16', 'district_17', 'district_18', 'district_19', 'district_20', 'district_20', 'district_21',\n",
    "                      'district_22', 'district_23', 'district_24', 'district_25', 'district_26', 'district_27', 'district_28', 'district_29', 'district_30', 'district_31',\n",
    "                      'district_32', 'district_33', 'district_34', 'district_35', 'district_36', 'district_37', 'district_38', 'district_39', 'district_40', 'district_41',\n",
    "                      'district_42', 'district_43', 'district_44', 'district_45', 'district_46', 'district_47', 'district_48', 'district_49', 'district_50',\n",
    "                      'election_code1', 'vote_type1', 'party_code1', 'election_code2', 'vote_type2', 'party_code2', 'election_code3', 'vote_type3', 'party_code3',\n",
    "                      'election_code4', 'vote_type4', 'party_code4', 'election_code5', 'vote_type5', 'party_code5', 'election_code6', 'vote_type6', 'party_code6',\n",
    "                      'election_code7', 'vote_type7', 'party_code7', 'election_code8', 'vote_type8', 'party_code8', 'election_code9', 'vote_type9', 'party_code9',\n",
    "                      'election_code10', 'vote_type10', 'party_code10', 'election_code11', 'vote_type11', 'party_code11', 'election_code12', 'vote_type12', 'party_code12', \n",
    "                      'election_code13', 'vote_type13', 'party_code13', 'election_code14', 'vote_type14', 'party_code14', 'election_code15', 'vote_type15', 'party_code15',\n",
    "                      'election_code16', 'vote_type16', 'party_code16', 'election_code17', 'vote_type17', 'party_code17', 'election_code18', 'vote_type18', 'party_code18',\n",
    "                      'election_code19', 'vote_type19', 'party_code19', 'election_code20', 'vote_type20', 'party_code20', 'election_code21', 'vote_type21', 'party_code21',\n",
    "                      'election_code22', 'vote_type22', 'party_code22', 'election_code23', 'vote_type23', 'party_code23', 'election_code24', 'vote_type24', 'party_code24',\n",
    "                      'election_code25', 'vote_type25', 'party_code25', 'election_code26', 'vote_type26', 'party_code26', 'election_code27', 'vote_type27', 'party_code27',\n",
    "                      'election_code28', 'vote_type28', 'party_code28', 'election_code29', 'vote_type29', 'party_code29', 'election_code30', 'vote_type30', 'party_code30',\n",
    "                      'election_code31', 'vote_type31', 'party_code31', 'election_code32', 'vote_type32', 'party_code32', 'election_code33', 'vote_type33', 'party_code33',\n",
    "                      'election_code34', 'vote_type34', 'party_code34', 'election_code35', 'vote_type35', 'party_code35', 'election_code36', 'vote_type36', 'party_code36',\n",
    "                      'election_code37', 'vote_type37', 'party_code37', 'election_code38', 'vote_type38', 'party_code38', 'election_code39', 'vote_type39', 'party_code39',\n",
    "                      'election_code40', 'vote_type40', 'party_code40', 'registration_date', 'party_affiliation_date', 'last_activity_date', 'precinct_group', 'phone_number',\n",
    "                      'ID_Compliant', 'Absentee_Category', 'Absentee_Category_Date', 'Ethnicity']\n",
    "voters_master_header = pd.DataFrame(columns=voters_master_columns)\n",
    "\n",
    "\n",
    "#cf1 and countidnumberpercf_1 transposed? wtf? consistent in test files... so far\n",
    "early_voter_columns = [ 'idnumber', 'name', 'precinct', 'ballot_party', 'early_voting_site', 'precinct_sub', 'activity_date', 'voter_party', 'mailing', 'ballot_style',\n",
    "                      'privacy_code', 'address', 'count_idnumber_per_cf_4', 'cf4', 'count_idnumber_per_cf_3', 'cf3', 'count_idnumber_per_cf_2', 'cf2', 'cf1', 'count_idnumber_per_cf_1',\n",
    "                      'countidnumber_per_repot', 'cf_party_selection', 'cf_site_selected', 'cf_ddate_range', 'cf_reg_party_selection']\n",
    "early_voter_header = pd.DataFrame(columns=early_voter_columns)\n",
    "\n",
    "ector_appraisal_columns = [ 'appraisal_year', 'account_number', 'aay_gis_parcel_id', 'aay_division_cdx', 'aay_prop_type_cd', 'aon_master_owner_id', 'multi_own_ind', \n",
    "                      'owner_name', 'owner_name2', 'owner_name3', 'address_line1', 'address_line2', 'city', 'state', 'zip', 'undivided_int_pct', 'ctm_imp_sptd_nm',\n",
    "                      'ctm_land_sptd_nm', 'ctm_prod_sptd_nm', 'LIF_TRANSFER_DT', 'LIF_TRANSFER_CDX', 'LIF_VOLUME_NUM', 'LIF_PAGE_NUM', 'LIF_INSTRUMENT_NUM'\n",
    "                      'LIF_ABSTRACT_NUM', 'LIF_SUBDIVISION_NAME', 'LIF_DESC_LINE1', 'LIF_DESC_LINE2', 'LIF_DESC_LINE3', 'LIF_DESC_LINE4', 'LIF_DESC_LINE5',\n",
    "                      'LIF_DESC_LINE6', 'LIF_DESC_LINE7', 'PROP_LOCATION', 'TOA_STREET_NUM', 'TOA_STREET_DIR', 'TOA_STREET_NAME', 'TOA_STREET_SUFFIX', 'AG_LAND_ACRES',\n",
    "                      'LAND_ACRES', 'LAND_SF', 'LAND_FF', 'AAY_LAND_VAL', 'AAY_IMPR_VAL', 'AAY_PERS_VAL', 'AAY_MNRL_VAL', 'AAY_TOT_VAL', 'AAY_PROD_LOSS_VAL', \n",
    "                      'AAY_CAP_LOSS_VAL', 'AAY_APPR_VAL', 'ACV_LAND_MKT_VAL', 'ACV_PROD_MKT_VAL', 'ACV_PROD_VAL', 'HS_EX_IND', 'O65_EX_IND', 'DIS_EX_IND', 'DV1_EX_IND',\n",
    "                      'DV2_EX_IND', 'ABT_EX_IND', 'HST_EX_IND', 'FPT_EX_IND', 'POL_EX_IND', 'TIF_EX_IND', 'SOL_EX_IND', 'WAT_EX_IND', 'TOT_EX_IND', 'PRO_EX_IND',\n",
    "                      'TOT_EX_TYP_CD', 'AAY_CITY_JURIS_CD', 'AAY_COUNTY_JURIS_CD', 'AAY_ISD_JURIS_CD', 'AAY_HOSPITAL_JURIS_CD', 'AAY_COLLEGE_JURIS_CD', 'AAY_SPEC_JURIS_CD',\n",
    "                      'AAY_SPEC2_JURIS_CD', 'AAY_SPEC3_JURIS_CD', 'AAY_SPEC4_JURIS_CD', 'AAY_SPEC5_JURIS_CD', 'CTM_CITY_JURIS_NM', 'CTM_HOSPITAL_JURIS_NM', 'CTM_SPEC2_JURIS_NM',\n",
    "                      'CTM_SPEC3_JURIS_NM', 'CTM_SPEC4_JURIS_NM', 'CTM_SPEC5_JURIS_NM', 'CTM_SIC_CDX', 'CTM_NBHD_CDX', 'CTM_P_BLDG_CLASS_CDX', 'YEAR_BUILT', 'AREA_SIZE',\n",
    "                      'VH1_EX_IND', 'LOW_EX_IND', 'GIT_EX_IND', 'PWR_EX_IND', 'AAY_MAPSCO_GRID', 'AAY_MAP_NUM', 'AAY_OVRLP_ACCOUNT_NUM', 'AAY_EXTRNL_ACCOUNT_NUM' ]\n",
    "ector_appraisal_header = pd.DataFrame(columns=ector_appraisal_columns)\n",
    "\n",
    "\n",
    "\n",
    "def clearList():\n",
    "    insert_list.clear()\n",
    "\n",
    "def insertDB_voterlist():\n",
    "    df_ector_voters_master = pd.DataFrame(insert_list, columns=voters_master_columns)\n",
    "    #print(df_w2_well_master)\n",
    "    df_ector_voters_master['VOTER_DATE'] = pd.to_datetime(df_ector_voters_master['VOTER_DATE'], format='%Y%m', errors='coerce')\n",
    "\n",
    "\n",
    "\n",
    "#limits for testing        \n",
    "maxct=10\n",
    "ct=0\n",
    "ct2=0\n",
    "\n",
    "for csvfile in os.listdir(\"..\\\\output\\\\temp\"):\n",
    "    #print(csvfile)\n",
    "    csvlist.clear()\n",
    "    reader = open(\"..\\\\output\\\\temp\\\\\" + csvfile, 'rb', newline=None)\n",
    "    data = reader.read().replace(b'\\r\\n', b'').splitlines()\n",
    "    binary_file = open(\"..\\\\output\\\\temp\\\\\" + csvfile, \"w\")\n",
    "    for line in lines:\n",
    "    if Limiting_Counter == True and ct > maxct:\n",
    "        break\n",
    "    #regex data extract by key **** testing\n",
    "    if re.match(r'^.}(\\d{2})}(\\d{6})}2023}04', line):\n",
    "        insert_list.append(line.rstrip('\\n').replace(' ', '').split('}'))\n",
    "        insertDB_voterlist()\n",
    "    clearList()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08974a12-2004-4ff8-89f0-ac64b6de55d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build correlation matrix for all gathered values\n",
    "\n",
    "corr = data[['voter_status', 'party_code', 'precinct', 'days_registered']].corr()\n",
    "\n",
    "plt.figure(figsize=(8, 4))\n",
    "mask = np.triu(np.ones_like(corr, dtype=np.bool))\n",
    "corr_plot = sns.heatmap(corr, mask=mask, annot=True, cmap='viridis')\n",
    "corr_plot.set_title(f\"Correlation matrix for {company_name}\")\n",
    "\n",
    "for item in corr_plot.get_xticklabels():\n",
    "    item.set_rotation(45)\n",
    "\n",
    "#plt.show()\n",
    "plt.subplots_adjust(left=0.15,bottom=0.25)\n",
    "plt.savefig('../results/Early_Voting_' + ev_date + 'Heatmap.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c3c1963-98fa-4723-a585-ef502b0f2421",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train tensorflow models for comments\n",
    "\n",
    "features_considered = ['']\n",
    "features = data[features_considered]\n",
    "\n",
    "data_number = features.shape[0]\n",
    "train_split = int(0.9 * data_number)\n",
    "\n",
    "dataset = features.values\n",
    "data_mean = dataset[:train_split].mean(axis=0)\n",
    "data_std = dataset[:train_split].std(axis=0)\n",
    "\n",
    "dataset = (dataset-data_mean)/data_std\n",
    "\n",
    "print(f\"mean normalized dataset of features = \\n\\n {dataset} \\n\")\n",
    "\n",
    "print(f\"shape of dataset = {dataset.shape}\")\n",
    "\n",
    "# batch generation algorithm\n",
    "\n",
    "def multivariate_data(dataset, target, start_index, end_index, history_size,\n",
    "                      target_size, step, single_step=False):\n",
    "    data = []\n",
    "    labels = []\n",
    "\n",
    "    start_index = start_index + history_size\n",
    "    if end_index is None:\n",
    "        end_index = len(dataset) - target_size\n",
    "\n",
    "    for i in range(start_index, end_index):\n",
    "        indices = range(i-history_size, i, step)\n",
    "        data.append(dataset[indices])\n",
    "\n",
    "        if single_step:\n",
    "            labels.append(target[i+target_size])\n",
    "        else:\n",
    "            labels.append(target[i:i+target_size])\n",
    "\n",
    "    return np.array(data), np.array(labels)\n",
    "\n",
    "# define partitions\n",
    "\n",
    "# INPUT: [to predict, past values used, prediction values, steps]\n",
    "\n",
    "labels = dataset[:, 0] #  getting company close\n",
    "n_past = 50 #  time stamps that will affect the future values (the more the more computing)\n",
    "n_prediction = 5 #  the time stamp in the future of which we want to find the prediction\n",
    "STEP = 1\n",
    "\n",
    "train_partitions, train_labels = multivariate_data(dataset, labels, 0, train_split, n_past, n_prediction, STEP, single_step=False)\n",
    "\n",
    "valid_partitions, valid_labels = multivariate_data(dataset, labels, train_split, None, n_past, n_prediction, STEP, single_step=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
